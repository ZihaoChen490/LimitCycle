{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.datasets.samples_generator module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.datasets. Anything that cannot be imported from sklearn.datasets is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import vstack, sqrt\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from torch.autograd import Variable\n",
    "import torch, time, sys,math\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score,mean_tweedie_deviance\n",
    "from torch import Tensor,nn\n",
    "from torch.nn import Linear,Sigmoid,ReLU, Module,ELU\n",
    "from torch.optim import SGD\n",
    "from torch.nn import MSELoss,CrossEntropyLoss\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "from Data_Utils import *\n",
    "from Plot_Utils import *\n",
    "from Math_Utils import *\n",
    "from sklearn.preprocessing import normalize\n",
    "import torch.nn.functional as F\n",
    "from transformer.Modules import ScaledDotProductAttention\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "# from torchdiffeq.torchdiffeq import odeint\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from torchdiffeq import odeint\n",
    "#data_sorce='C:/Aczh work place/3_paper/algo_new/data/'\n",
    "#argv='C:/Aczh work place/3_paper/algonew/experiment-pend/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap(x, out_min, out_max):\n",
    "    in_min, in_max = np.min(x), np.max(x)\n",
    "    return (x - in_min) * (out_max - out_min) / (in_max - in_min) + out_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "beats = np.load('control_beats_6.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LcODE(Module):\n",
    "    def __init__(self, n_inputs):\n",
    "        super(LcODE, self).__init__()\n",
    "        self.hidden1 = Linear(n_inputs, 60)\n",
    "        kaiming_uniform_(self.hidden1.weight)\n",
    "        self.act1 = ELU()\n",
    "        self.hidden2 = Linear(60, 30)\n",
    "        kaiming_uniform_(self.hidden2.weight)\n",
    "        self.act2 = ELU()\n",
    "        self.hidden3 = Linear(30, 10)\n",
    "        kaiming_uniform_(self.hidden3.weight)\n",
    "        self.act3 = ELU()\n",
    "        self.hidden4 = Linear(10, n_inputs)\n",
    "        kaiming_uniform_(self.hidden4.weight)\n",
    "        self.act4 = Sigmoid()\n",
    "        self.nfe = 0\n",
    "    def forward(self,t, X):\n",
    "        self.nfe+=1\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "        X = self.hidden2(X)\n",
    "        X = self.act2(X)\n",
    "        X = self.hidden3(X)\n",
    "        X=self.act3(X)\n",
    "        X = self.hidden4(X)\n",
    "        X=self.act4(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_inner, n_head, d_k, d_v, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)\n",
    "\n",
    "    def forward(self, enc_input, slf_attn_mask=None):\n",
    "        enc_output, enc_slf_attn = self.slf_attn(\n",
    "            enc_input, enc_input, enc_input, mask=slf_attn_mask)\n",
    "        enc_output = self.pos_ffn(enc_output)\n",
    "        return enc_output, enc_slf_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.w_qs = nn.Linear(d_model, n_head * d_k, bias=False)\n",
    "        self.w_ks = nn.Linear(d_model, n_head * d_k, bias=False)\n",
    "        self.w_vs = nn.Linear(d_model, n_head * d_v, bias=False)\n",
    "        self.fc = nn.Linear(n_head * d_v, d_model, bias=False)\n",
    "        self.attention = ScaledDotProductAttention(temperature=d_k ** 0.5)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n",
    "        sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)\n",
    "        residual = q\n",
    "        # Pass through the pre-attention projection: b x lq x (n*dv)\n",
    "        # Separate different heads: b x lq x n x dv\n",
    "        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)\n",
    "        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)\n",
    "        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)\n",
    "        # Transpose for attention dot product: b x n x lq x dv\n",
    "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)   # For head axis broadcasting.\n",
    "        q, attn = self.attention(q, k, v, mask=mask)\n",
    "        # Transpose to move the head dimension back: b x lq x n x dv\n",
    "        # Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)\n",
    "        q = q.transpose(1, 2).contiguous().view(sz_b, len_q, -1)\n",
    "        q = self.dropout(self.fc(q))\n",
    "        q += residual\n",
    "        q = self.layer_norm(q)\n",
    "        return q, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, temperature, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        attn = torch.matmul(q / self.temperature, k.transpose(2, 3))\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask == 0, -1e9)\n",
    "        attn = self.dropout(F.softmax(attn, dim=-1))\n",
    "        output = torch.matmul(attn, v)\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_in, d_hid, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.w_1 = nn.Linear(d_in, d_hid) # position-wise\n",
    "        self.w_2 = nn.Linear(d_hid, d_in) # position-wise\n",
    "        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.w_2(F.relu(self.w_1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x += residual\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_beats(beats_npy = beats, \n",
    "                   ntotal = len(beats[0]) - 1,\n",
    "                   nsample = 200,\n",
    "                   noise_std = 0.00,\n",
    "                  ): \n",
    "    orig_ts = np.linspace(0, 1, num=len(beats_npy[0])-1)\n",
    "    samp_ts = orig_ts[:nsample]\n",
    "    xs = np.array([np.diff(single_beat) for single_beat in beats_npy])\n",
    "    ys = np.array([single_beat[:-1] for single_beat in beats_npy])\n",
    "    orig_trajs = np.stack((xs, ys), axis=2)#[:1000]\n",
    "    samp_trajs = []\n",
    "    for i in range(len(orig_trajs)):\n",
    "        orig_traj = orig_trajs[i]\n",
    "        orig_traj = remap(orig_traj, -1, 1)\n",
    "        orig_trajs[i] = orig_traj\n",
    "        samp_traj = orig_traj.copy()\n",
    "        idx0 = np.random.randint(ntotal - nsample)\n",
    "        samp_traj = samp_traj[idx0:idx0+nsample]\n",
    "        samp_traj += npr.randn(*samp_traj.shape) * noise_std\n",
    "        samp_trajs.append(samp_traj)\n",
    "    # batching for sample trajectories is good for RNN; batching for original\n",
    "    # trajectories only for ease of indexing\n",
    "    orig_trajs = np.stack(orig_trajs, axis=0)\n",
    "    samp_trajs = np.stack(samp_trajs, axis=0)\n",
    "    return orig_trajs, samp_trajs, orig_ts, samp_ts\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5000\n",
    "latent_dim = 50\n",
    "nhidden = 50\n",
    "rnn_nhidden = 20\n",
    "obs_dim = 2\n",
    "nspiral = N #len(beats)\n",
    "noise_std = 0.005\n",
    "ntotal = 499\n",
    "cutoff = 100\n",
    "nsample = ntotal - cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCNRL:\n",
    "    def __init__(self,Dataset,steps,gap):\n",
    "        self.Dataset=Dataset\n",
    "        self.steps=steps\n",
    "        self.gap=gap\n",
    "        self.lam=0.5\n",
    "        self.lr=0.000001\n",
    "        self.weight_decay=1e-5\n",
    "        self.epoch=15\n",
    "        self.NewIndex, self.TargetSet = np.loadtxt('NewIndex.txt'),np.loadtxt('TargetSet.txt')\n",
    "        if self.NewIndex is None:\n",
    "            self.NewIndex,self.TargetSet=ReIndex(Dataset.train_dl[:len(Dataset.train_dl)//10],gap,num_model)\n",
    "\n",
    "    def graph(self,Index):\n",
    "        if Index>0:\n",
    "            self.Node_list=list(set(self.NewIndex[Index-1]+self.NewIndex[Index]+self.NewIndex[Index+1]))\n",
    "        else:\n",
    "            self.Node_list=list(set(self.NewIndex[Index]+self.NewIndex[Index+1]))\n",
    "        lis=[]\n",
    "        for i in self.Node_list:\n",
    "            for j in self.Node_list:\n",
    "                if i!=j and (i in self.NewIndex[j] or j in self.NewIndex[i]):\n",
    "                    lis.append((i,j,{'weight':(self.Dataset.train_dl[i,:]-self.Dataset.train_dl[j,:]).std()}))\n",
    "        self.G=nx.Graph(lis)\n",
    "        self.A=nx.adjacency_matrix(self.G)\n",
    "        nx.degree(self.G)\n",
    "        self.L=nx.normalized_laplacian_matrix(self.G)\n",
    "        self.eig = np.linalg.eigvals(self.L.A)\n",
    "        print(self.eig)\n",
    "        return self.eig\n",
    "\n",
    "    def creat_gauss_kernel(self,kernel_size=3, sigma=1, k=1):\n",
    "        if sigma == 0:\n",
    "            sigma = ((kernel_size - 1) * 0.5 - 1) * 0.3 + 0.8\n",
    "        X = np.linspace(-k, k, kernel_size)\n",
    "        Y = np.linspace(-k, k, kernel_size)\n",
    "        x, y = np.meshgrid(X, Y)\n",
    "        x0 = 0\n",
    "        y0 = 0\n",
    "        gauss = 1 / (2 * np.pi * sigma ** 2) * np.exp(- ((x - x0) ** 2 + (y - y0) ** 2) / (2 * sigma ** 2))\n",
    "        return gauss\n",
    "\n",
    "\n",
    "    def train_sto_mlp_model(self,num_model,steps=200,gap=10):\n",
    "        model = [MLP(DS.dim) for i in range(num_model)]\n",
    "        train_dl=self.Dataset.train_dl\n",
    "        print(train_dl.shape)\n",
    "        data_all = []\n",
    "        loss_res = []\n",
    "        torch.manual_seed(8)\n",
    "        np.random.seed(5)\n",
    "        optimizer = [torch.optim.Adam(model[i].parameters(), lr=self.lr, weight_decay=self.weight_decay) for i in range(num_model)]\n",
    "        data=torch.from_numpy(train_dl)\n",
    "        regularization_loss,t = 0,len(train_dl)//steps\n",
    "        print('each length of the dataset is',t)\n",
    "        for epoch in range(self.epoch):\n",
    "            #for step in range(steps//num_model):\n",
    "            for step in range(steps-70):\n",
    "                print('......................................................................')\n",
    "                print('Now we strat to train the',step,'th step in epoch:', epoch, 'with total step:', steps)\n",
    "                for rd in range(t-gap):\n",
    "                    inputs=[data[(step)*t+rd,:] for i in range(num_model)]\n",
    "                    #NewIndex, TargetSet = ReIndex(train_dl, gap, num_model,(step)*t+rd)\n",
    "                    targets=[torch.from_numpy(self.TargetSet[int(self.NewIndex[(step)*t+rd][i]),:]) for i in range(num_model)]\n",
    "                    #inputs = [data[(step*num_model+i)*t+rd,:] for i in range(num_model)]\n",
    "                    #targets=[data[(step*num_model+i)*t+rd+gap,:] for i in range(num_model)]\n",
    "                    [optimizer[i].zero_grad() for i in range(num_model)]\n",
    "                    yhat = [model[i](inputs[i]) for i in range(num_model)]\n",
    "                    loss = [(1/(1+abs(inputs[i]-targets[i]).std()))*abs(L2_loss(yhat[i], targets[i]) -self.lam * torch.norm(yhat[i] - sum(yhat) /len(model))) for i in range(len(model))]\n",
    "                    #loss = [(L2_loss(yhat[i], targets[i])) for i in range(num_model)]\n",
    "                    [loss[i].backward(retain_graph=True) for i in range(num_model)]\n",
    "                    [optimizer[i].step() for i in range(num_model)]\n",
    "                print('Now the loss=',loss)\n",
    "            loss_res.append([loss[i].detach().numpy() for i in range(len(model))])\n",
    "            for i in range(len(model)):\n",
    "                torch.save(model[i],'C:/Aczh work place/3_paper/SCNRL_Master/model/CellCycle/model_ito_weight' + str(i) + '.pkl')\n",
    "                print('model have been saved')\n",
    "            if epoch>4 and (sum([(loss_res[epoch-1][i]-loss_res[epoch][i]) for i in range(len(model))]))<1e-10:\n",
    "                break\n",
    "        try:\n",
    "            data_a=self.plot_model(1000,model,True)\n",
    "            data_all.append(data_a[-1][-1])\n",
    "        except:\n",
    "            print('Unsave model')\n",
    "        return data_all\n",
    "    def plot_train(self,train_dl,dim):\n",
    "        time=self.Dataset.time[0:len(self.Dataset.time):gap].reshape(-1,1)\n",
    "        print(len(time))\n",
    "        train_data=self.Dataset.transform_back(self.Dataset.train_dl,'train2back')\n",
    "        plt.plot(time, train_data[:len(self.Dataset.train_data)//10, dim])\n",
    "        plt.show()\n",
    "        plot_vector_fields(train_dl,dim)\n",
    "\n",
    "\n",
    "    def evaluate_model_on_test(self,model,step=60,gap=1):\n",
    "        predictions, actuals = list(), list()\n",
    "        data = torch.from_numpy(self.Dataset.test_dl)\n",
    "        t=1\n",
    "        print('test_dl is',len(self.Dataset.test_dl))\n",
    "        for i in range((len(self.Dataset.test_dl)-gap)//10):\n",
    "            inputs = data[i * t, :]\n",
    "            targets = data[i * t + gap, :]\n",
    "            yhat = sum([model[i](inputs) for i in range(len(model))])/len(model)\n",
    "            yhat = yhat.detach().numpy()\n",
    "            actual = targets.numpy()\n",
    "#        actual = actual.reshape((len(actual), 2))\n",
    "            predictions.append(yhat)\n",
    "            actuals.append(actual)\n",
    "        predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "        mse = mean_squared_error(actuals, predictions)\n",
    "        #MAE\n",
    "        return mse\n",
    "    def load_model(self,save_path):\n",
    "        model = [MLP(DS.dim) for i in range(num_model)]\n",
    "        for i in range(num_model):\n",
    "            model[i] = torch.load(save_path+'model_ito_l1_' + str(i) + '.pkl')\n",
    "        return model\n",
    "\n",
    "    def evaluate_model_ori_data(self, path,gap=1):\n",
    "        model=self.load_model(path)\n",
    "        predictions, actuals = list(), list()\n",
    "        data= torch.from_numpy(self.Dataset.ori_data)\n",
    "        t = 1\n",
    "        print('ori_data is', self.Dataset.transform_back(self.Dataset.ori_data.shape,'ori2back'))\n",
    "        for i in range(self.Dataset.ori_data.shape[1]):\n",
    "            inputs = data[i * t, :]\n",
    "            targets = data[i * t + gap, :]\n",
    "            yhat = sum([model[i](inputs) for i in range(len(model))]) / len(model)\n",
    "            yhat = yhat.detach().numpy()\n",
    "            actual = targets.numpy()\n",
    "            #actual = actual.reshape((len(actual), 2))\n",
    "            predictions.append(yhat)\n",
    "            actuals.append(actual)\n",
    "        predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "        mse = mean_squared_error(actuals, predictions)\n",
    "        return mse\n",
    "\n",
    "    def plot_model(self,step, model, flag=False):\n",
    "        ax1 = plt.subplot(1, 1, 1)\n",
    "        path_all = []\n",
    "        print('step:', step)\n",
    "        x_n = [[[0.05 * j, 0.05 * i] for i in range(21)] for j in range(21)]\n",
    "        for i in range(len(x_n)):\n",
    "            for j in range(len(x_n[0])):\n",
    "                x_0 = x_n[j][i]\n",
    "                print('第', i, j, '个', 'x_0 is :', x_0)\n",
    "                pre_path = []\n",
    "                for k in range(step):\n",
    "                    pre_path.append(list(x_0))\n",
    "                    yhat = predict(x_0, model)\n",
    "                    x_0 = yhat[0]\n",
    "                path_all.append(pre_path)\n",
    "        for path in path_all:\n",
    "            plt.plot([i[0] for i in path], [i[1] for i in path])\n",
    "            print('res', path)\n",
    "        if flag:\n",
    "            plt.show()\n",
    "        return path_all\n",
    "\n",
    "    def plot_time_series_model(self, model,steps,gap,dim, flag=False):\n",
    "        ax1 = plt.subplot(1, 1, 1)\n",
    "        time=DS.time[0:len(DS.time):2*gap].reshape(-1,1)\n",
    "        print(len(time))\n",
    "        ori_data=self.Dataset.transform_back(self.Dataset.ori_data,'ori2back')\n",
    "        #plt.show()\n",
    "        x_0=self.Dataset.ori_data[0,:]\n",
    "        path_all = []\n",
    "        for i in range(len(time)):\n",
    "            x_0 = np.hstack([self.Dataset.ori_data[i,:],time[i,:]])\n",
    "            yhat = predict(x_0, model)\n",
    "            #path_all.append(list(self.Dataset.transform_back(yhat[0],'train2back')))\n",
    "            path_all.append(yhat[0])\n",
    "        for dim in range(45):\n",
    "            print(dim)\n",
    "            plt.plot(time, ori_data[:len(self.Dataset.ori_data)//20:gap, dim])\n",
    "            plt.show()\n",
    "            plt.plot(time,[i[dim] for i in path_all])\n",
    "            plt.show()\n",
    "        print('Over Now')\n",
    "        return path_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successly load time\n",
      "(1400000, 44) (600000, 44) (100000, 44)\n",
      "self.train_dl (1400000, 44)\n"
     ]
    }
   ],
   "source": [
    "num_model=8\n",
    "data_path='C:/Aczh work place/3_paper/SNCRL_Dataset/CellCycle/'\n",
    "DS=Dataset(data_path,1)\n",
    "steps,gap=140,10\n",
    "SC=SCNRL(DS,steps,gap)\n",
    "save_path='C:/Aczh work place/3_paper/SCNRL_Master/model/CellCycle/'\n",
    "#SC.train_sto_mlp_model(num_model,steps,gap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successly load time\n",
      "(1400000, 44) (600000, 44) (100000, 44)\n",
      "self.train_dl (1400000, 44)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LCrnn' object has no attribute 'plot_time_series_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-7f4d66718174>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m#data_all=SC.plot_model(1000,model,True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mdata_all\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_time_series_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgap\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'LCrnn' object has no attribute 'plot_time_series_model'"
     ]
    }
   ],
   "source": [
    "from baseline.lcrnn_model import *\n",
    "\n",
    "def load_model(save_path):\n",
    "    model = torch.load(save_path)\n",
    "    return model\n",
    "\n",
    "steps=140\n",
    "gap=10\n",
    "data_path='C:/Aczh work place/3_paper/SNCRL_Dataset/CellCycle/'\n",
    "DS=Dataset(data_path,1)\n",
    "LC=LCrnn(DS,steps,gap)\n",
    "save_path='C:/Aczh work place/3_paper/SCNRL_Master/baseline/lcrnn_model_0.pkl'\n",
    "model = load_model(save_path)\n",
    "dim=1\n",
    "#data_all=SC.plot_model(1000,model,True)\n",
    "data_all=LC.plot_time_series_model(model,steps,gap,dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
